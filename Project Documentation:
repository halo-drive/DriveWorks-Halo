Project Documentation:

directory explanations:
-'.idea' = pycharm ide configuration files especially within the project folder.
-'.venv' = virtual environment for any python project to run, contains all the packages 
- classes.py = contains all the class ids (in form of dictionary to the )
- main.py - main file which incorporates all the scripts and does the inference and starts the project(on the terminal run `python3 main.py`)
- mcmcontrol.py - handles mcm control command api, needed to creating mcm objects
- braking.py/ steer.py - handles braking/steering control uses mcmcontrol objects
- requirements.txt - has a list of all the project dependecies


Note: before launching up the project please activate the virtual environment which loads all the dependencies

follow-ups to establish the project:

1.Set up a YOLOv8 object detection model to detect stop signs from a camera feed.
2.Created a virtual CAN bus (vcan0) to simulate communication with a vehicle's braking system.
3.Integrated the object detection model with a CAN bus braking system to apply brakes when a stop sign is detected.
4.Implemented dynamic brake control where the braking force is proportional to the confidence level of the stop sign detection.
5.Added lidar processing to visualize lidar data alongside the camera feed.
6.Refined the code to improve performance and address potential issues like lag and unresponsive windows.
7.Explored options for using multiple cameras for object detection, including stitching frames and performing separate inference.
8.Discussed strategies for testing the stop sign detection and braking system in a safe and controlled environment.
9.Addressed various error messages and provided solutions to resolve them.
10.Modularized the code into classes and functions for better organization and reusability.
11.Incorporated logging to record important events and debug the application.


remove cuda and nvidia packages:
sudo dpkg -r --force-all $(dpkg -l | grep cuda | awk '{print $2}')
sudo dpkg -r --force-all $(dpkg -l | grep nvidia | awk '{print $2}')


install fresh cuda and nvidia drivers: (12.4) + 565.57.01
please do it over network deb file for faster setup and easy dependency resolver: 
	* sudo wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.1-1_all.deb
	* sudo dpkg -i cuda-keyring_1.1-1_all.deb
	* sudo apt-get update
	* sudo apt-get -y install cuda-toolkit-12-4

installing tensorrt 10.3 for 20.04 focal with upto CUDA 12.5 support
download - nv-tensorrt-local-repo-ubuntu2004-10.3.0-cuda-12.5_1.0-1_amd64.deb
add new repo key : 	sudo cp /var/nv-tensorrt-local-repo-ubuntu2004-10.3.0-cuda-12.5/nv-tensorrt-local-3628E411-keyring.gpg /usr/share/keyrings/
create repo file in /etc/apt/sources.list.d/ : echo "deb [signed-by=/usr/share/keyrings/nv-tensorrt-local-3628E411-keyring.gpg] file:/var/nv-tensorrt-local-repo-ubuntu2004-10.3.0-cuda-12.5 /" | sudo tee /etc/apt/sources.list.d/tensorrt.list
update package list: sudo apt update
install tensorrt : sudo apt install tensorrt

start exporting the model to TRT engine format: 
yolo export model=yolov8lane.pt format=engine device=0 imgsz=640,640 half=True workspace=4
ln -s /usr/lib/python3/dist-packages/tensorrt* ~/Desktop/YoloDetection/venv/lib/python3.10/site-packages/


check for installed versions of dependecies:
python3 -c "import torch; print(torch.version.cuda)"
python3 -c "import tensorrt; print(tensorrt.__version__)"

export PATH=/usr/src/tensorrt/bin${PATH:+:${PATH}}
training for lane detection
TuSimple dataset only provides annotations for the 20th frame of each video sequence, which explains why we have ~3.6K labeled images out of 72K total images.
covert tusimple data format to yolo train format
yolo task=segment mode=train model=yolov8x-seg.pt data=dataset/tusimple.yaml epochs=100 imgsz=640

python3 lanedetection.py --model runs/segment/train/weights/best.pt --conf-thres 0.6 --iou-thres 0.5 --width 800 --height 600 --source video.MOV







===============Building Drive Environment for GXO=====================


venv status: /home/pomo/Desktop/YoloDetection/.venv/bin/python /home/pomo/Desktop/YoloDetection/checkenv.py 
Python version: 3.10.15 (main, Sep  7 2024, 18:35:33) [GCC 9.4.0]
Torch version: 2.5.0+cu124
CUDA available: True
CUDA version: 12.4
GPU device: NVIDIA GeForce RTX 3060
TensorRT version: 10.3.0


--we would need to create a different venv for the project to run because of varied parameters such as:
provided platform -- aarch64
OS - NV Yocto built of Ubuntu 20.04
TRT version - NV optimised 8.6.13
CUDA - NV optimised 11.4
Torch - NV optimised torch2.0.0-nv23.04

Considering the different discripancies in the environemnt there are packages that would need to be built from scratch, especially the ones that are direct or indirect dependency matchers of the NVIDIA optimised packeges.


listing out the pip installed pacakges on the host machine would let us know what are the pip packages that are actually needed to run the project.
remember the below listed packages for the project to run on RTX 3060 x86 host machine:
aenum==3.1.15
argparse-addons==0.12.0
bitstruct==8.19.0
can-isotp==1.8
cantools==36.2.0
certifi==2024.8.30
charset-normalizer==3.4.0
colorama==0.4.6
coloredlogs==15.0.1
commonmark==0.9.1
config==0.5.1
contourpy==1.1.1
crc8==0.2.1
crccheck==1.3.0
cycler==0.12.1
diskcache==5.6.3
filelock==3.16.1
flatbuffers==24.3.25
fonttools==4.54.1
fsspec==2024.10.0
humanfriendly==10.0
idna==3.10
importlib_resources==6.4.5
Jinja2==2.10.1
kiwisolver==1.4.7
MarkupSafe==2.1.5
matplotlib==3.7.5
mpmath==1.3.0
msgpack==1.0.8
networkx==3.1
numpy==1.24.4
nvidia-cublas-cu12==12.1.3.1
nvidia-cuda-cupti-cu12==12.1.105
nvidia-cuda-nvrtc-cu12==12.1.105
nvidia-cuda-runtime-cu12==12.1.105
nvidia-cudnn-cu12==9.1.0.70
nvidia-cufft-cu12==11.0.2.54
nvidia-curand-cu12==10.3.2.106
nvidia-cusolver-cu12==11.4.5.107
nvidia-cusparse-cu12==12.1.0.106
nvidia-nccl-cu12==2.20.5
nvidia-nvjitlink-cu12==12.6.77
nvidia-nvtx-cu12==12.1.105
onnx==1.17.0
onnxruntime-gpu==1.19.2
onnxslim==0.1.36
opencv-python==4.10.0.84
packaging==24.1
pandas==2.0.3
pillow==10.4.0
pkg_resources==0.0.0
platformdirs==4.3.6
protobuf==5.28.3
psutil==6.1.0
py-cpuinfo==9.0.0
pycuda==2024.1.2
Pygments==2.18.0
pyparsing==3.1.4
python-can==3.3.4
python-dateutil==2.9.0.post0
pytools==2024.1.14
pytz==2024.2
PyYAML==6.0.2
requests==2.32.3
rich==11.0.0
scipy==1.10.1
seaborn==0.13.2
six==1.16.0
sympy==1.13.3
tensorrt==10.3.0
tensorrt-cu12==10.3.0
tensorrt-cu12-bindings==10.3.0
tensorrt-cu12-libs==10.3.0
textparser==0.24.0
torch==2.4.1
torchvision==0.19.1
tqdm==4.66.6
triton==3.0.0
typing_extensions==4.12.2
tzdata==2024.2
ultralytics==8.3.27
ultralytics-thop==2.0.10
urllib3==2.2.3
wrapt==1.16.0
zipp==3.20.2

The important packages that need to be built from scratch are so far recognised to be: 
torch (wheel already provided by NVIDIA, (refer to Drive AGX Orin forum platform))
torchvision (is required the pytorch model visualisation effects)
onnnxruntime-gpu - required if dealing to onnx runtime at the same time insdie the SoC, there are no prebuilt binaries 					   for this package especialy for aarch64 machines
opencv-python - needs to be built from source to improve performance bottleneck
pycuda.drive - pytorch models use the pycuda.driver package to shift the inference into the cuda device



check the camera connected or not via cv2 lib:
python3 -c "import cv2; cap = cv2.VideoCapture(0); print('USB Camera Connected' if cap.isOpened() else 'No USB Camera Found'); cap.release()"

the cv2.VideoCapture() defines the index of the camera that is connected, these indices are to be verified by enlistsing the videoX(X being the index) inside the /dev/ (devices) directory.



exporting the model on the SoC:
/usr/src/tensorrt/bin/trtexec \
    --onnx=./yolov8x-seg.onnx \
    --saveEngine=./yolov8x-seg_fp16.trt \
    --fp16 \
    --workspace=4096 \
    --verbose \
    --minShapes=images:1x3x320x320 \
    --optShapes=images:1x3x640x640 \
    --maxShapes=images:1x3x1280x1280


note that the above export isnt the dynamic input paramters, this means that we are actually defining the image resolution for this, which means input shapes are defined by us. if it dynamic this means the model rescales and decides the resolution for the inference, this is hassle as it leads more time investing in removing false positives and post processing each different frames.
a non dynamic approach fixed for optimal inference resolution can actaully help fasten up the computer vision pipeline as it makes the image resolution corrected before feeding it into the model 
